# Discrete probability distributions

A **discrete probability distribution** is the probability distribution of a random variable that can take on only a countable number of values (almost surely) which means that the probability of any event $E$ can be expressed as a (finite or countably infinite) sum: 

\begin{equation}
  P(X\in E)=\sum _{\omega \in A}P(X=\omega),
  (\#eq:prob-dist-discrete)
\end{equation}

where $A$ is a countable set and $\omega$ is an *elementary event*. Thus the discrete random variables are exactly those with a **probability mass function** $p ( x ) = P ( X = x )$ [@enwiki-prob-dist]. 



## Bernoulli distribution {#bernoulli}

Random variables with a **Bernoulli distribution** (sometimes *0-1-distribution*) are used to describe random events that take the value of $1$ ("success") with probability $p$ and the value of $0$ ("failure") with probability $q = 1-p$.

Correspondingly, a **Bernoulli trial** (or *binomial trial*) is a random experiment with exactly two possible outcomes, "success" and "failure", in which the probability of success is the same every time the experiment is conducted.

| Bernoulli distribution |   |
|:---------------------|:--------------------------------------------------------|
|*Parameters*|$0 \leq p \leq 1, \quad q = 1 − p$|
|*Notation*|${\displaystyle X\sim {\mathcal {B}}(p)}$ oder $X\sim Ber_{p}$|
|*PMF*|${\displaystyle f(x\mid p)={\begin{cases}p^{x}(1-p)^{1-x}&{\text{falls}}\quad x=0,1\\0&{\text{sonst.}}\end{cases}}}$|
|*CDF*|${\displaystyle F_{X}(x)={\begin{cases}0&{\text{ falls }}\quad x<0\\1-p&{\text{ falls }}\quad 0\leq x<1\\1&{\text{ falls }}\quad x\geq 1\end{cases}}}.$|
|*Expectation*|${E}\left(X\right)=p$|
|*Variance*|${V}\left(X\right)=pq$|
|*Example 1*|Flipping a coin: Head ("success"), $p=1/2$, and tail ("failure"), $q=1/2$.|
|*Example 2*|Throwing a die, whereat only „6“ is regarded as a success: $p=1/6$, $q=5/6$.|

Eine Reihe von unabhängigen identischen Versuchen, bei der jeder Einzelversuch der Bernoulli-Verteilung genügt, wird ***Bernoulli-Prozess*** oder *bernoullisches Versuchsschema* genannt.

## Binominalverteilung

Die Binomialverteilung beschreibt die Anzahl der Erfolge in einem **Bernoulli-Prozess** (see \@ref(bernoulli)), also einer Serie von gleichartigen und unabhängigen Versuchen, die jeweils genau zwei mögliche Ergebnisse haben („Erfolg“ oder „Misserfolg“). 

| Bernoulli-Verteilung |   |
|:---------------------|:--------------------------------------------------------|
|*Notation*| $X\sim B_{n,p}$ |
|*PMF*| $P(X=k\mid n,p) =  {\displaystyle {\begin{cases}{\binom {n}{k}}p^{k}(1-p)^{n-k}&{\text{falls}}\quad k\in \left\{0,1,\dots ,n\right\}\\0&{\text{sonst.}}\end{cases}}}$ |


## Geometrische Verteilung

Werden nacheinander *unabhängige Bernoulli-Experimente* durchgeführt, d.h. es gibt die Versuchsausgänge $1$ (Erfolg, z.B. Kopf) und $0$ (Missverfolg, z.B. Zahl), dann ordnet die geometrische Verteilung entweder

- **Variante A**: der Anzahl $X$ der Bernoulli-Versuche, bis zum ersten Erfolg (definiert auf der Menge  $\mathbb {N}$), bzw.
- **Variante B**: der Anzahl $Y$ der Fehlversuche vor dem ersten Erfolg (definiert auf der Menge $\mathbb {N} _{0}$)

Wahrscheinlichkeiten zu. Theoretisch können dabei unendlich viele Bernoulli-Experimente nötig sein. Die beiden Varianten stehen in der Beziehung $X=Y+1$. 

**Derivation**: *Da die Experimente unabhängig sind, müssen damit $P(X=n)$ alle Versuche vor dem $n$-ten Versuch müssen gescheitert sein und der $n$-te Versuch muss erfolgreich gewesen sein. Die Wahrscheinlichkeitsfunktion ist demanach $P(X=n) = (1-p)^{n-1} p$.*

| Geometric distribution |   |
|:---------------------|:--------------------------------------------------------|
|*Parameter*|${\displaystyle 0<p\leq 1}$ success probability (real)|
|**Definition / Verteilungsfunktion**| $X \sim G(p)$ , wenn für die Wahrscheinlichkeit, dass man genau $n$ Versuche benötigt, um zum ersten Erfolg zu kommen, gilt $${\displaystyle \operatorname {P} (X=n)=p(1-p)^{n-1}},$$ bzw. $Y \sim G(p)$, wenn für die Wahrscheinlichkeit, $n$ Fehlversuche vor dem ersten Erfolg zu haben, gilt $$\operatorname {P}(Y=n)=p(1-p)^{{n}}, \;n=0,1,2,\dotsc .$$|
|**Verteilungsfunktion (A)**|$${\displaystyle \operatorname {P} (X\leq n)=1-q^{n}=1-(1-p)^{n}}$$|
|**Verteilungsfunktion (B)**|$${\displaystyle \operatorname {P} (Y\leq n)=1-q^{n+1}=1-(1-p)^{n+1}}$$|
|**Erwartungswert (A)**|$$\operatorname {E}(X)={\frac {1}{p}}$$|
|**Erwartungswert (B)**|$$\operatorname {E}(Y)=\operatorname {E}(X)-1={\frac {1-p}{p}}$$|
|**Varianz**|$$\operatorname {Var}(X)=\operatorname {Var}(Y)={\frac {1-p}{p^{2}}}={\frac {1}{p^{{2}}}}-{\frac {1}{p}}$$|

**Gedächtnislosigkeit**: Die geometrische Verteilung ist eine gedächtnislose Verteilung, d. h., es gilt für

- Variante A: $\operatorname {P}(X=n+k\,|\,X>n)=\operatorname {P}(X=k)\quad n,k=1,2,\dotsc$
- Variante B: $\operatorname {P}(Y=n+k\,|\,Y\geq n)=\operatorname {P}(Y=k)\quad n,k=0,1,2,\dotsc$

Ist also von einer geometrisch verteilten Zufallsvariablen bekannt, dass sie größer als der Wert $n$ ist (Variante A) bzw. mindestens den Wert $n$ hat (Variante B), so ist die Wahrscheinlichkeit, dass sie diesen Wert um $k$ übertrifft, genau so groß wie die, dass eine identische Zufallsvariable überhaupt den Wert $k$ annimmt.

Die Gedächtnislosigkeit ist eine definierende Eigenschaft; die geometrische Verteilung ist also die einzig mögliche gedächtnislose diskrete Verteilung. Ihr stetiges Pendant hierbei ist die *Exponentialverteilung* (\@ref(exponentialverteilung)). 


### R: Zähldichte (probability mass function)

**Beispiel**: Wie oft muss ich beim *Mensch-ärgere-dich-nicht-Spiel* auf die erste Sechs warten? 

Die Wahrscheinlichkeit, dass ich genau 4 Versuche benötige ($P(X=4)$) bzw. 3 Mal warten muss (d.h. 3 Misserfolg habe, $P(Y=3)$), ist:

```{r density geometric distribution}
dgeom(x = 3, prob = 1/6, log = FALSE)
```

Entsprechend ergibt sich für die Anzahl der Misserfolge/Würfe vor dem ersten Erfolg bzw. die Wartenzeit im Bereich  $[0, 12]$ dann folgende **Zähldiche** bzw. **probability mass function** (PMF):

```{r geometric-dist, fig.cap='Zähldichte der geometrischen Verteilung.', out.width='80%', fig.asp=.75, fig.align='center'}
x   <- 0:12                                              # Choose the sample space (here, it's 0,1,2,...,10)
p_x <- dgeom(x = x, prob = 1/6)                          # Compute the shape of the distribution

plot(x, p_x, type="h", xlim=c(0,12), ylim=c(0,max(p_x)), # Plot the Geometric probability dist
     lwd = 8, col = "blue",ylab = "P (Y = n)", xlab = "n")
```


### R: Verteilungsfunktion (CDF)

Um nun die Wahrscheinlichkeit für höchstens drei Misserfolge vor dem ersten Erflog zu finden -- also dafür spätestens beim vierten Mal die $6$ zu erhalten -- benötigt man die **Verteilungsfunktion** bzw. CDF:

```{r}
pgeom(q = 3, prob = 1/6)
```

Entsprechend ergibt sich für die Wahrscheinlichekiten, dass die Wartezeit kleiner bzw. gleich $0, 1, ...$ ($P(Y\leq n)$) ist dann folgende CDF:

```{r geometric-cdf, fig.cap='Verteilungsfunktion (CDF) der geometrischen Verteilung.', out.width='80%', fig.asp=.75, fig.align='center'}
x   <- 0:12                      # choose the sample space (here, it's 0,1,2,...,10)
Fx <- pgeom(q = x, prob = 1/6)   # compute the shape of the distribution
n <- length(x)

plot(x = NA, y = NA, pch = NA, xlim = c(0, max(x)), ylim = c(0, 1),
     xlab = "n", ylab = "P (Y <= n)")

points(x = x[-n], y = Fx[-n], pch=19, col = "blue")
points(x = x[-1], y = Fx[-n], pch=1, col = "blue")
for(i in 1:(n-1)) points(x=x[i+0:1], y=Fx[c(i,i)], type="l", col = "blue")
```




## Poisson distribution

The **Poisson distribution** expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.

***Derivation:*** The Poisson distribution can be derived from the binomial distribution: Consider a call center that receives seven calls per hour on average. Let's now divide the hour into 60 minutes, each representing a Bernoulli trial that yields a value of $1$ if somebody calls, and $0$ if not. The number of calls per hour $X$ can then be modeled as a binomial distributed random variable $X \sim B(n,p)$, where $n=60$. We obtain the success probability $p$ using the expectation of the binomial distribution $E[X]=np$ which equals $7$ (calls) in our case, i.e. $7=np=60p$ and hence $p=\frac{7}{60}$. Therefore, $X \sim B(60, \frac{7}{60})$, and $X \sim B(n, \frac{7}{n})$ for an partition of the hour into $n$ even segments.

*For an arb*



random experiment that The number of calls that falls into a particular $X$ can then be modelled  RV $X$ Let's divide the hour into 60 minutes.  The probability that $B()$  as the limit of $k$ (successes) if $n \to \infty$ (draws) from the binominal distribution. 

| Poisson distribution |   |
|:---------------------|:--------------------------------------------------------|
|*Notation*|$X \sim \text{Pois}(\lambda)$|
|*PMF*|$P(X=k) = {\displaystyle {\frac {\lambda ^{k}e^{-\lambda }}{k!}}}$|
|*Example 1*|The number of decay events that occur from a radioactive source during a defined observation period.|
|*Example 2*|Chewing gum on a sidewalk. The number of chewing gums on a single tile is approximately Poisson distributed.|
|*Youtube*|[https://www.youtube.com/watch?v=IrHiPYOdEAo](https://www.youtube.com/watch?v=IrHiPYOdEAo)|

### PMF example in R

### CDF example in R



## Multinominal distribution

The **multinomial distribution** models the probability of counts for each side of a $k$-sided dice rolled $n$ times. For $n$ independent trials each of which leads to a success for exactly one of $k$ categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.

## Probability distributions in R

**R** nutzt standatisierte Präfixe, um verschiedene Verteilungseigenschaften abzufragen:

| Präfix  | Bedeutung                                 | Math. Ausdruck |
|:-----|:---------------------------|:-----------------|
| `d`     | Zähldiche (*PMF*) bzw. Wahrscheinlichkeitsdichte  (*PDF*)   | $P(X=x)$       |
| `p`     | Verteilungsfunktion (*CDF*) | $P(X \leq x)$  |
| `q`     | Quantilsfunktion                         | Given $p$, the smallest $x$ such that $P(X\leq x) > p$|
| `r`     | Ziehen aus gegebener Verteilung                         | |

`q` is hence the inverse of the *CDF* ($P(X\leq x)$) und damit für eine entsprechende Wahrscheinlichkeit $p$ dasjenige $x$, sodass die Wkt., dass $X\leq x$ gleich $p$ ist.


